{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PARSING etherscan.io\n",
    "\n",
    "I made an attempt to parse more full information about user's transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Modules to reload:\n",
      "all-except-skipped\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from user_agent import generate_user_agent\n",
    "from application.load_transaction_data import load_ether_data, load_token_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "transactions_df = load_ether_data()\\\n",
    "    .query(\"Status != 'Error(0)'\")\n",
    "token_df = load_token_data()\n",
    "\n",
    "transactions_transfers_df = pd.read_pickle(\"data/transactions_transfers_df\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1110/7418 [19:24<2:01:13,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '1 wei From Wrapped'\n",
      "Error on 0x90c67a86473a56f543d0984e569e60f4b70ea85a623342c822bc0b4d5821ce7e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 5398/7418 [2:30:23<1:22:55,  2.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '90 wei From SushiSwap: Router To  0x99fd1378ca799ed6772fe7bcdc9b30b389518962'\n",
      "Error on 0x428c9d824d6aa0c2599ee48be07592efd37c5eac7ea66a8bce9678f537dbdd3d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7418/7418 [4:00:43<00:00,  1.95s/it]  \n"
     ]
    }
   ],
   "source": [
    "def export_transfer_list(transfer: str):\n",
    "    # try:\n",
    "    #     amount = re.findall(r\"(\\d+\\.\\d+|\\d+\\,\\d+)\", transfer)[0]\n",
    "    # except IndexError:\n",
    "    #     amount = re.findall(r\"\\d+\", transfer)[0]\n",
    "    amount = float(transfer.split(\"TRANSFER\")[1].split(\"Ether\")[0].strip(\" \").replace(\",\", \"\"))\n",
    "    from_ = re.findall(r\"From (.*?) To\", transfer)[0]\n",
    "    to_ = transfer.split(\"  \")[-1]\n",
    "    return amount, from_, to_\n",
    "\n",
    "\n",
    "def extract_token_info(token_url):\n",
    "    result_dict = {}\n",
    "\n",
    "    # Parse from and to\n",
    "    try:\n",
    "        result_dict['from'] =  token_url.find(\"span\", class_='hash-tag text-truncate hash-tag-custom-from tooltip-address').text\n",
    "    except Exception as e:\n",
    "        result_dict['from'] = e\n",
    "    try:\n",
    "        result_dict['to'] = token_url.find(\"span\", class_='hash-tag text-truncate hash-tag-custom-to tooltip-address').text\n",
    "    except Exception as e:\n",
    "        result_dict['to'] = e\n",
    "\n",
    "    # Parse USD value. Check, where or not does USD value exist on this page?\n",
    "    try:\n",
    "        token1_usd = re.findall(r\"\\(\\$.*?\\)\", str(token_url))[0]\n",
    "        token1_usd = token1_usd.strip(\"()\").replace(\" (\", \"\").replace(\"$\", \"\")\n",
    "        result_dict['usd'] = float(token1_usd.replace(\",\", \"_\"))\n",
    "        result_dict['amount'] = token_url.find(\"span\", 'data-toggle'=='tooltip').text\n",
    "        result_dict['amount'] = float(result_dict['amount'].replace(\",\", \"\"))\n",
    "    except (AttributeError, IndexError) as e:\n",
    "        result_dict['usd'] = 'empty'\n",
    "        result_dict['amount'] = token_url.find_all(\"span\", class_='mr-1')[-1].text\n",
    "        result_dict['amount'] = float(result_dict['amount'].replace(\",\", \"\"))\n",
    "\n",
    "    # Parse token hash in order to parse full info later\n",
    "    try:\n",
    "        result_dict['token_hash'] = token_url.find(\"a\").get(\"href\").split(\"?\")[0].split(\"/\")[2]\n",
    "    except Exception as e:\n",
    "        result_dict['token_hash'] = e\n",
    "    return result_dict\n",
    "\n",
    "def extract_tokens_info(transaction_hash):\n",
    "    req = requests.get(f\"https://etherscan.io/tx/{transaction_hash}\",\n",
    "                   headers={\"User-Agent\": generate_user_agent()})\n",
    "    content = req.content\n",
    "    soup = BeautifulSoup(content)\n",
    "    tokens_df = []\n",
    "    bad_tokens = []\n",
    "    errors = []\n",
    "    all_tokens = soup.find_all(\"li\", class_='media align-items-baseline mb-2')\n",
    "    for index, tokens in enumerate(all_tokens):\n",
    "        try:\n",
    "            tokens_df.append(\n",
    "                pd.DataFrame.from_dict(extract_token_info(all_tokens[index]), orient='index').T\n",
    "            )\n",
    "        except Exception as e:\n",
    "            bad_tokens.append(index)\n",
    "            errors.append(e)\n",
    "    try:\n",
    "\n",
    "        tokens_df = pd.concat(tokens_df)\n",
    "    except ValueError:\n",
    "        tokens_df = pd.DataFrame(data=[transaction_hash], columns=['Txhash'])\n",
    "\n",
    "    tokens_df.index = np.arange(tokens_df.shape[0])\n",
    "    # TRANSFER parsing\n",
    "    transfer_text_list = []\n",
    "    transfers = soup.find_all(\"li\", class_='media align-items-baseline')\n",
    "    if len(transfers) > 0:\n",
    "        for transf in transfers:\n",
    "            transfer_text_list.append(transf.text.replace(\"\\xa0\", \"\"))\n",
    "\n",
    "        transfer_df = pd.DataFrame(list(map(export_transfer_list, transfer_text_list)), columns=['transfer_amount', 'transfer_from', 'transfer_to'])\n",
    "        tokens_df = pd.concat([tokens_df, transfer_df], axis=1)\n",
    "\n",
    "    tokens_df['Txhash'] = transaction_hash\n",
    "    return tokens_df\n",
    "\n",
    "all_tokens_df = []\n",
    "transactions_list = set(transactions_df['Txhash'])\n",
    "for tx_hash in tqdm(transactions_list):\n",
    "    try:\n",
    "        time.sleep(random.uniform(0, 1))\n",
    "        all_tokens_df.append(extract_tokens_info(tx_hash))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error on {tx_hash}\")\n",
    "    pd.concat(all_tokens_df).to_pickle(\"transactions_transfers_df\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing token names according to hash"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [03:34<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "all_token_hash = set(transactions_transfers_df['token_hash'].dropna())\n",
    "def parsing_tokens(token_hash):\n",
    "    token_dict = {}\n",
    "    token_dict['token_hash'] = token_hash\n",
    "    req = requests.get(f\"https://etherscan.io/token/{token_hash}\",\n",
    "                   headers={\"User-Agent\": generate_user_agent()})\n",
    "    content = req.content\n",
    "    soup = BeautifulSoup(content)\n",
    "    try:\n",
    "        token_dict['token_short_name'] = soup.find(\"span\", class_='text-secondary small').text\n",
    "    except Exception as e:\n",
    "        token_dict['token_short_name'] = e\n",
    "    try:\n",
    "        token_dict['token_long_name'] = soup.find(\"a\", class_='mb-1 mb-sm-0 u-label u-label--xs u-label--info').text\n",
    "    except Exception as e:\n",
    "        token_dict['token_long_name'] = e\n",
    "    return token_dict\n",
    "\n",
    "token_names_df = []\n",
    "for token in tqdm(all_token_hash):\n",
    "    time.sleep(random.uniform(0, 1))\n",
    "    try:\n",
    "        token_names_df.append(\n",
    "                pd.DataFrame.from_dict(parsing_tokens(token), orient='index').T)\n",
    "    except Exception as e:\n",
    "        token_names_df.append(pd.DataFrame(data=[token], columns=['Txhash']))\n",
    "token_names_df = pd.concat(token_names_df)\n",
    "token_names_df.to_pickle(\"data/token_hash_name\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}